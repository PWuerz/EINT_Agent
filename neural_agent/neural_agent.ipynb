{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Mapping, Any, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import textworld\n",
    "import textworld.gym\n",
    "from textworld import EnvInfos\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Neural Agent\n",
    "class CommandScorer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(CommandScorer, self).__init__()\n",
    "        torch.manual_seed(42)  # For reproducibility\n",
    "        self.embedding    = nn.Embedding(input_size, hidden_size)\n",
    "        self.encoder_gru  = nn.GRU(hidden_size, hidden_size)\n",
    "        self.cmd_encoder_gru  = nn.GRU(hidden_size, hidden_size)\n",
    "        self.state_gru    = nn.GRU(hidden_size, hidden_size)\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.state_hidden = torch.zeros(1, 1, hidden_size, device=device)\n",
    "        self.critic       = nn.Linear(hidden_size, 1)\n",
    "        self.att_cmd      = nn.Linear(hidden_size * 2, 1)\n",
    "\n",
    "    def forward(self, obs, commands, **kwargs):\n",
    "        input_length = obs.size(0)\n",
    "        batch_size = obs.size(1)\n",
    "        nb_cmds = commands.size(1)\n",
    "\n",
    "        embedded = self.embedding(obs)\n",
    "        encoder_output, encoder_hidden = self.encoder_gru(embedded)\n",
    "        state_output, state_hidden = self.state_gru(encoder_hidden, self.state_hidden)\n",
    "        self.state_hidden = state_hidden\n",
    "        value = self.critic(state_output)\n",
    "\n",
    "        # Attention network over the commands.\n",
    "        cmds_embedding = self.embedding.forward(commands)\n",
    "        _, cmds_encoding_last_states = self.cmd_encoder_gru.forward(cmds_embedding)  # 1 x cmds x hidden\n",
    "\n",
    "        # Same observed state for all commands.\n",
    "        cmd_selector_input = torch.stack([state_hidden] * nb_cmds, 2)  # 1 x batch x cmds x hidden\n",
    "\n",
    "        # Same command choices for the whole batch.\n",
    "        cmds_encoding_last_states = torch.stack([cmds_encoding_last_states] * batch_size, 1)  # 1 x batch x cmds x hidden\n",
    "\n",
    "        # Concatenate the observed state and command encodings.\n",
    "        cmd_selector_input = torch.cat([cmd_selector_input, cmds_encoding_last_states], dim=-1)\n",
    "\n",
    "        # Compute one score per command.\n",
    "        scores = F.relu(self.att_cmd(cmd_selector_input)).squeeze(-1)  # 1 x Batch x cmds\n",
    "\n",
    "        probs = F.softmax(scores, dim=2)  # 1 x Batch x cmds\n",
    "        index = probs[0].multinomial(num_samples=1).unsqueeze(0) # 1 x batch x indx\n",
    "        return scores, index, value\n",
    "\n",
    "    def reset_hidden(self, batch_size):\n",
    "        self.state_hidden = torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "class NeuralAgent:\n",
    "    \"\"\" Simple Neural Agent for playing TextWorld games. \"\"\"\n",
    "    MAX_VOCAB_SIZE = 1000\n",
    "    UPDATE_FREQUENCY = 10\n",
    "    LOG_FREQUENCY = 500\n",
    "    GAMMA = 0.9\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self._initialized = False\n",
    "        self._epsiode_has_started = False\n",
    "        self.id2word = [\"<PAD>\", \"<UNK>\"]\n",
    "        self.word2id = {w: i for i, w in enumerate(self.id2word)}\n",
    "        \n",
    "        self.model = CommandScorer(input_size=self.MAX_VOCAB_SIZE, hidden_size=128)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), 0.00003)\n",
    "        \n",
    "        self.mode = \"test\"\n",
    "    \n",
    "    def train(self):\n",
    "        self.mode = \"train\"\n",
    "        self.stats = {\"max\": defaultdict(list), \"mean\": defaultdict(list)}\n",
    "        self.transitions = []\n",
    "        self.model.reset_hidden(1)\n",
    "        self.last_score = 0\n",
    "        self.no_train_step = 0\n",
    "    \n",
    "    def test(self):\n",
    "        self.mode = \"test\"\n",
    "        self.model.reset_hidden(1)\n",
    "        \n",
    "    @property\n",
    "    def infos_to_request(self) -> EnvInfos:\n",
    "        return EnvInfos(description=True, inventory=True, admissible_commands=True,\n",
    "                        has_won=True, has_lost=True)\n",
    "    \n",
    "    def _get_word_id(self, word):\n",
    "        if word not in self.word2id:\n",
    "            if len(self.word2id) >= self.MAX_VOCAB_SIZE:\n",
    "                return self.word2id[\"<UNK>\"]\n",
    "            \n",
    "            self.id2word.append(word)\n",
    "            self.word2id[word] = len(self.word2id)\n",
    "            \n",
    "        return self.word2id[word]\n",
    "            \n",
    "    def _tokenize(self, text):\n",
    "        # Simple tokenizer: strip out all non-alphabetic characters.\n",
    "        text = re.sub(\"[^a-zA-Z0-9\\- ]\", \" \", text)\n",
    "        word_ids = list(map(self._get_word_id, text.split()))\n",
    "        return word_ids\n",
    "\n",
    "    def _process(self, texts):\n",
    "        texts = list(map(self._tokenize, texts))\n",
    "        max_len = max(len(l) for l in texts)\n",
    "        padded = np.ones((len(texts), max_len)) * self.word2id[\"<PAD>\"]\n",
    "\n",
    "        for i, text in enumerate(texts):\n",
    "            padded[i, :len(text)] = text\n",
    "\n",
    "        padded_tensor = torch.from_numpy(padded).type(torch.long).to(device)\n",
    "        padded_tensor = padded_tensor.permute(1, 0) # Batch x Seq => Seq x Batch\n",
    "        return padded_tensor\n",
    "      \n",
    "    def _discount_rewards(self, last_values):\n",
    "        returns, advantages = [], []\n",
    "        R = last_values.data\n",
    "        for t in reversed(range(len(self.transitions))):\n",
    "            rewards, _, _, values = self.transitions[t]\n",
    "            R = rewards + self.GAMMA * R\n",
    "            adv = R - values\n",
    "            returns.append(R)\n",
    "            advantages.append(adv)\n",
    "            \n",
    "        return returns[::-1], advantages[::-1]\n",
    "\n",
    "    def act(self, obs: str, score: int, done: bool, infos: Mapping[str, Any]) -> Optional[str]:\n",
    "        \n",
    "        # Build agent's observation: feedback + look + inventory.\n",
    "        input_ = \"{}\\n{}\\n{}\".format(obs, infos[\"description\"], infos[\"inventory\"])\n",
    "        \n",
    "        # Tokenize and pad the input and the commands to chose from.\n",
    "        input_tensor = self._process([input_])\n",
    "        commands_tensor = self._process(infos[\"admissible_commands\"])\n",
    "        \n",
    "        # Get our next action and value prediction.\n",
    "        outputs, indexes, values = self.model(input_tensor, commands_tensor)\n",
    "        action = infos[\"admissible_commands\"][indexes[0]]\n",
    "        \n",
    "        if self.mode == \"test\":\n",
    "            if done:\n",
    "                self.model.reset_hidden(1)\n",
    "            return action\n",
    "        \n",
    "        self.no_train_step += 1\n",
    "        \n",
    "        if self.transitions:\n",
    "            reward = score - self.last_score  # Reward is the gain/loss in score.\n",
    "            self.last_score = score\n",
    "            if infos[\"has_won\"]:\n",
    "                reward += 100\n",
    "            if infos[\"has_lost\"]:\n",
    "                reward -= 100\n",
    "                \n",
    "            self.transitions[-1][0] = reward  # Update reward information.\n",
    "        \n",
    "        self.stats[\"max\"][\"score\"].append(score)\n",
    "        if self.no_train_step % self.UPDATE_FREQUENCY == 0:\n",
    "            # Update model\n",
    "            returns, advantages = self._discount_rewards(values)\n",
    "            \n",
    "            loss = 0\n",
    "            for transition, ret, advantage in zip(self.transitions, returns, advantages):\n",
    "                reward, indexes_, outputs_, values_ = transition\n",
    "                \n",
    "                advantage        = advantage.detach() # Block gradients flow here.\n",
    "                probs            = F.softmax(outputs_, dim=2)\n",
    "                log_probs        = torch.log(probs)\n",
    "                log_action_probs = log_probs.gather(2, indexes_)\n",
    "                policy_loss      = (-log_action_probs * advantage).sum()\n",
    "                value_loss       = (.5 * (values_ - ret) ** 2.).sum()\n",
    "                entropy     = (-probs * log_probs).sum()\n",
    "                loss += policy_loss + 0.5 * value_loss - 0.1 * entropy\n",
    "                \n",
    "                self.stats[\"mean\"][\"reward\"].append(reward)\n",
    "                self.stats[\"mean\"][\"policy\"].append(policy_loss.item())\n",
    "                self.stats[\"mean\"][\"value\"].append(value_loss.item())\n",
    "                self.stats[\"mean\"][\"entropy\"].append(entropy.item())\n",
    "                self.stats[\"mean\"][\"confidence\"].append(torch.exp(log_action_probs).item())\n",
    "            \n",
    "            if self.no_train_step % self.LOG_FREQUENCY == 0:\n",
    "                msg = \"{}. \".format(self.no_train_step)\n",
    "                msg += \"  \".join(\"{}: {:.3f}\".format(k, np.mean(v)) for k, v in self.stats[\"mean\"].items())\n",
    "                msg += \"  \" + \"  \".join(\"{}: {}\".format(k, np.max(v)) for k, v in self.stats[\"max\"].items())\n",
    "                msg += \"  vocab: {}\".format(len(self.id2word))\n",
    "                print(msg)\n",
    "                self.stats = {\"max\": defaultdict(list), \"mean\": defaultdict(list)}\n",
    "            \n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.model.parameters(), 40)\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "        \n",
    "            self.transitions = []\n",
    "            self.model.reset_hidden(1)\n",
    "        else:\n",
    "            # Keep information about transitions for Truncated Backpropagation Through Time.\n",
    "            self.transitions.append([None, indexes, outputs, values])  # Reward will be set on the next call\n",
    "        \n",
    "        if done:\n",
    "            self.last_score = 0  # Will be starting a new episode. Reset the last score.\n",
    "        \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import gym\n",
    "import textworld.gym\n",
    "\n",
    "\n",
    "def play(agent, path, max_step=50, nb_episodes=10, verbose=True):\n",
    "    infos_to_request = agent.infos_to_request\n",
    "    infos_to_request.max_score = True  # Needed to normalize the scores.\n",
    "    \n",
    "    gamefiles = [path]\n",
    "    if os.path.isdir(path):\n",
    "        gamefiles = glob(os.path.join(path, \"*.ulx\"))\n",
    "        \n",
    "    env_id = textworld.gym.register_games(gamefiles,\n",
    "                                          request_infos=infos_to_request,\n",
    "                                          max_episode_steps=max_step)\n",
    "    env = gym.make(env_id)  # Create a Gym environment to play the text game.\n",
    "    if verbose:\n",
    "        if os.path.isdir(path):\n",
    "            print(os.path.dirname(path), end=\"\")\n",
    "        else:\n",
    "            print(os.path.basename(path), end=\"\")\n",
    "        \n",
    "    # Collect some statistics: nb_steps, final reward.\n",
    "    avg_moves, avg_scores, avg_norm_scores = [], [], []\n",
    "    for no_episode in range(nb_episodes):\n",
    "        obs, infos = env.reset()  # Start new episode.\n",
    "\n",
    "        score = 0\n",
    "        done = False\n",
    "        nb_moves = 0\n",
    "        while not done:\n",
    "            command = agent.act(obs, score, done, infos)\n",
    "            obs, score, done, infos = env.step(command)\n",
    "            nb_moves += 1\n",
    "        \n",
    "        agent.act(obs, score, done, infos)  # Let the agent know the game is done.\n",
    "                \n",
    "        if verbose:\n",
    "            print(\".\", end=\"\")\n",
    "        avg_moves.append(nb_moves)\n",
    "        avg_scores.append(score)\n",
    "        avg_norm_scores.append(score / infos[\"max_score\"])\n",
    "\n",
    "    env.close()\n",
    "    msg = \"  \\tavg. steps: {:5.1f}; avg. score: {:4.1f} / {}.\"\n",
    "    if verbose:\n",
    "        if os.path.isdir(path):\n",
    "            print(msg.format(np.mean(avg_moves), np.mean(avg_norm_scores), 1))\n",
    "        else:\n",
    "            print(msg.format(np.mean(avg_moves), np.mean(avg_scores), infos[\"max_score\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tw-cooking-recipe1+take1-11Oeig8bSVdGSp78.ulx..........  \tavg. steps:  34.8; avg. score:  1.2 / 3.\n"
     ]
    }
   ],
   "source": [
    "# We report the score and steps averaged over 10 playthroughs.\n",
    "agent = NeuralAgent()\n",
    "#agent = RandomAgent()\n",
    "play(agent, \"sample_games/tw-cooking-recipe1+take1-11Oeig8bSVdGSp78.ulx\")    # Dense rewards\n",
    "#play(agent, \"sample_games/tw-cooking-recipe2+take2+cut+open-BnYEixa9iJKmFZxO.ulx\") # Balanced rewards\n",
    "#play(agent, \"sample_games/tw-cooking-recipe3-aXjNc96rIaD9Fk93.ulx\")   # Sparse rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "500. reward: -1.513  policy: -17.617  value: 236.147  entropy: 2.948  confidence: 0.054  score: 2  vocab: 197\n",
      "1000. reward: -1.080  policy: -13.705  value: 189.866  entropy: 2.990  confidence: 0.052  score: 2  vocab: 203\n",
      "1500. reward: -0.629  policy: -5.606  value: 155.816  entropy: 2.954  confidence: 0.053  score: 3  vocab: 207\n",
      "2000. reward: -1.080  policy: -13.386  value: 174.001  entropy: 2.998  confidence: 0.051  score: 2  vocab: 211\n",
      "2500. reward: -0.411  policy: -4.778  value: 262.636  entropy: 2.971  confidence: 0.052  score: 3  vocab: 216\n",
      "3000. reward: -1.513  policy: -16.856  value: 290.618  entropy: 2.957  confidence: 0.053  score: 3  vocab: 221\n",
      "3500. reward: -0.178  policy: -1.680  value: 208.199  entropy: 2.937  confidence: 0.054  score: 3  vocab: 224\n",
      "4000. reward: -0.627  policy: -6.822  value: 169.639  entropy: 2.961  confidence: 0.053  score: 3  vocab: 227\n",
      "4500. reward: -1.522  policy: -21.221  value: 391.401  entropy: 2.979  confidence: 0.052  score: 3  vocab: 231\n",
      "5000. reward: -1.082  policy: -15.815  value: 203.182  entropy: 2.934  confidence: 0.055  score: 2  vocab: 231\n",
      "5500. reward: -0.420  policy: -1.722  value: 127.304  entropy: 2.913  confidence: 0.056  score: 3  vocab: 232\n",
      "6000. reward: -1.082  policy: -11.296  value: 140.060  entropy: 2.969  confidence: 0.053  score: 2  vocab: 232\n",
      "6500. reward: -0.864  policy: -8.628  value: 119.759  entropy: 2.943  confidence: 0.054  score: 2  vocab: 233\n",
      "7000. reward: -0.642  policy: -8.577  value: 111.332  entropy: 2.952  confidence: 0.054  score: 2  vocab: 233\n",
      "7500. reward: -0.189  policy: -4.564  value: 124.992  entropy: 2.925  confidence: 0.055  score: 3  vocab: 234\n",
      "8000. reward: -0.193  policy: -1.643  value: 27.294  entropy: 2.914  confidence: 0.056  score: 2  vocab: 234\n",
      "8500. reward: 0.033  policy: -1.216  value: 88.610  entropy: 2.934  confidence: 0.055  score: 3  vocab: 235\n",
      "9000. reward: -0.411  policy: -6.217  value: 134.239  entropy: 2.948  confidence: 0.054  score: 3  vocab: 236\n",
      "9500. reward: -0.638  policy: -7.398  value: 97.706  entropy: 2.946  confidence: 0.055  score: 2  vocab: 236\n",
      "10000. reward: 0.024  policy: -1.010  value: 65.418  entropy: 2.905  confidence: 0.057  score: 3  vocab: 236\n",
      "10500. reward: -0.640  policy: -11.777  value: 140.186  entropy: 2.925  confidence: 0.056  score: 3  vocab: 236\n",
      "11000. reward: -0.413  policy: -7.606  value: 93.889  entropy: 2.943  confidence: 0.054  score: 2  vocab: 237\n",
      "11500. reward: 0.256  policy: 3.042  value: 40.220  entropy: 2.965  confidence: 0.053  score: 3  vocab: 237\n",
      "12000. reward: -0.633  policy: -10.650  value: 135.545  entropy: 2.920  confidence: 0.057  score: 2  vocab: 237\n",
      "12500. reward: -0.636  policy: -7.695  value: 101.035  entropy: 2.981  confidence: 0.053  score: 2  vocab: 237\n",
      "13000. reward: -0.636  policy: -2.392  value: 102.908  entropy: 2.976  confidence: 0.052  score: 3  vocab: 239\n",
      "13500. reward: -0.636  policy: -8.808  value: 113.291  entropy: 2.975  confidence: 0.053  score: 2  vocab: 239\n",
      "14000. reward: -0.171  policy: -7.522  value: 213.771  entropy: 2.960  confidence: 0.054  score: 3  vocab: 239\n",
      "14500. reward: -0.196  policy: -6.844  value: 117.394  entropy: 2.991  confidence: 0.052  score: 3  vocab: 239\n",
      "15000. reward: -0.629  policy: -10.928  value: 231.275  entropy: 2.934  confidence: 0.055  score: 3  vocab: 239\n",
      "15500. reward: 0.047  policy: -0.999  value: 234.420  entropy: 2.988  confidence: 0.052  score: 3  vocab: 239\n",
      "16000. reward: -0.402  policy: -5.356  value: 127.577  entropy: 2.916  confidence: 0.057  score: 3  vocab: 239\n",
      "16500. reward: -0.402  policy: -2.975  value: 133.654  entropy: 2.934  confidence: 0.056  score: 3  vocab: 239\n",
      "17000. reward: -0.862  policy: -12.179  value: 148.568  entropy: 2.940  confidence: 0.055  score: 2  vocab: 239\n",
      "17500. reward: -0.629  policy: -8.884  value: 141.737  entropy: 2.969  confidence: 0.054  score: 2  vocab: 239\n",
      "18000. reward: -0.633  policy: -14.434  value: 243.873  entropy: 2.977  confidence: 0.054  score: 3  vocab: 239\n",
      "18500. reward: -0.864  policy: -12.155  value: 146.837  entropy: 2.914  confidence: 0.057  score: 3  vocab: 239\n",
      "19000. reward: -0.420  policy: -4.813  value: 64.355  entropy: 2.891  confidence: 0.059  score: 2  vocab: 239\n",
      "19500. reward: -0.189  policy: -3.149  value: 109.504  entropy: 2.931  confidence: 0.056  score: 3  vocab: 239\n",
      "20000. reward: 0.044  policy: -1.791  value: 232.980  entropy: 2.920  confidence: 0.056  score: 3  vocab: 239\n",
      "20500. reward: 1.396  policy: 16.710  value: 323.511  entropy: 2.996  confidence: 0.053  score: 3  vocab: 239\n",
      "Trained in 1140.13 secs\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "agent = NeuralAgent()\n",
    "\n",
    "print(\"Training\")\n",
    "agent.train()  # Tell the agent it should update its parameters.\n",
    "starttime = time()\n",
    "#play(agent, \"./games/rewardsDense_goalDetailed.ulx\", nb_episodes=500, verbose=False)  # Dense rewards game.\n",
    "play(agent, \"sample_games/tw-cooking-recipe1+take1-11Oeig8bSVdGSp78.ulx\", nb_episodes=500, verbose=False)    # Dense rewards\n",
    "#play(agent, \"sample_games/tw-cooking-recipe2+take2+cut+open-BnYEixa9iJKmFZxO.ulx\", nb_episodes=500, verbose=False) # Balanced rewards\n",
    "#play(agent, \"sample_games/tw-cooking-recipe3-aXjNc96rIaD9Fk93.ulx\", nb_episodes=500, verbose=False)   # Sparse rewards\n",
    "\n",
    "print(\"Trained in {:.2f} secs\".format(time() - starttime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tw-cooking-recipe1+take1-11Oeig8bSVdGSp78.ulx..........  \tavg. steps:  40.9; avg. score:  1.5 / 3.\n"
     ]
    }
   ],
   "source": [
    "# We report the score and steps averaged over 10 playthroughs.\n",
    "agent.test()\n",
    "play(agent, \"sample_games/tw-cooking-recipe1+take1-11Oeig8bSVdGSp78.ulx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tw-venv",
   "language": "python",
   "name": "tw-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
